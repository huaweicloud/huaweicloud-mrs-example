hoodie.datasource.write.recordkey.field=id
hoodie.datasource.write.partitionpath.field=job
hoodie.delete.shuffle.parallelism=100
hoodie.upsert.shuffle.parallelism=100
hoodie.bulkinsert.shuffle.parallelism=100
hoodie.insert.shuffle.parallelism=100
hoodie.finalize.write.parallelism=100
hoodie.cleaner.parallelism=100
#hive config
hoodie.datasource.hive_sync.table=hudi_deltastreamer_example_table
hoodie.datasource.hive_sync.partition_fields=job
hoodie.datasource.hive_sync.assume_date_partitioning=false
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor
hoodie.datasource.hive_sync.use_jdbc=false

# Kafka Source topic
hoodie.deltastreamer.source.kafka.topic=testtopic

# Kafka props
# The kafka cluster we want to ingest from
bootstrap.servers= xx.xx.xx.xx:xxxx
auto.offset.reset=earliest
#auto.offset.reset=latest
group.id=hoodie-delta-streamer
offset.rang.limit=10000